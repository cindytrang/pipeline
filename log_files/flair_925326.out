Label encoding mapping:
missing_label: 0
non-predator: 1
predator: 2

---            Run settings            ---
     model_indicator: bert_classifier
             seq_len: 512
   dataset_indicator: PAN12
             project: flair
              run_id: 2024-04-20_23-15-33__bert_classifier_on_PAN12_with_seq-len-512
            data_dir: datasets/PAN12/
             run_dir: resources/2024-04-20_23-15-33__bert_classifier_on_PAN12_with_seq-len-512/
2024-04-20 23:15:33,163 Reading data from datasets/PAN12
2024-04-20 23:15:33,163 Train: datasets/PAN12/balanced_training_data.csv
2024-04-20 23:15:33,163 Dev: None
2024-04-20 23:15:33,164 Test: datasets/PAN12/testing_data.csv
2024-04-20 23:16:27,734 Filtering empty sentences
2024-04-20 23:27:23,407 Corpus: 67335 train + 7482 dev + 155102 test sentences
Corpus Statistics: {
    "TRAIN": {
        "dataset": "TRAIN",
        "total_number_of_documents": 6734,
        "number_of_documents_per_class": {
            "0.0": 6712,
            "0.20091760616484394": 1,
            "0.01092657937176568": 1,
            "0.03873252683717413": 1,
            "0.22925358299492177": 1,
            "0.01421319865847107": 1,
            "0.05088806868987229": 1,
            "0.04255271933823552": 1,
            "0.15431557436114204": 1,
            "0.033963367381603445": 1,
            "0.03386415324057118": 1,
            "0.1683442401340428": 1,
            "0.13349866779820807": 1,
            "0.017162442064271227": 1,
            "0.003968109286082118": 1,
            "0.03634830386378241": 1,
            "0.04689317186751587": 1,
            "0.03496363026443669": 1,
            "0.14787663729770237": 1,
            "0.11737583070669137": 1,
            "0.020842748416879858": 1,
            "0.6972504121533641": 1,
            "0.03163033546382873": 1
        },
        "number_of_tokens_per_tag": {},
        "number_of_tokens": {
            "total": 6734,
            "min": 1,
            "max": 1,
            "avg": 1.0
        }
    },
    "TEST": {
        "dataset": "TEST",
        "total_number_of_documents": 15510,
        "number_of_documents_per_class": {
            "label": 1,
            "predator": 3742,
            "non-predator": 2849,
            "missing_label": 8918
        },
        "number_of_tokens_per_tag": {},
        "number_of_tokens": {
            "total": 1756362,
            "min": 1,
            "max": 74605,
            "avg": 113.24061895551257
        }
    },
    "DEV": {
        "dataset": "DEV",
        "total_number_of_documents": 748,
        "number_of_documents_per_class": {
            "0.0": 744,
            "0.1648859293631806": 1,
            "0.0321844321595628": 1,
            "0.05488505761691375": 1,
            "0.03353190232606149": 1
        },
        "number_of_tokens_per_tag": {},
        "number_of_tokens": {
            "total": 748,
            "min": 1,
            "max": 1,
            "avg": 1.0
        }
    }
}
2024-04-20 23:30:37,962 Computing label dictionary. Progress:
2024-04-20 23:30:38,308 Dictionary created for label 'class' with 24 values: 0.0 (seen 6712 times), 0.20091760616484394 (seen 1 times), 0.01092657937176568 (seen 1 times), 0.03873252683717413 (seen 1 times), 0.22925358299492177 (seen 1 times), 0.01421319865847107 (seen 1 times), 0.05088806868987229 (seen 1 times), 0.04255271933823552 (seen 1 times), 0.15431557436114204 (seen 1 times), 0.033963367381603445 (seen 1 times), 0.03386415324057118 (seen 1 times), 0.1683442401340428 (seen 1 times), 0.13349866779820807 (seen 1 times), 0.017162442064271227 (seen 1 times), 0.003968109286082118 (seen 1 times), 0.03634830386378241 (seen 1 times), 0.04689317186751587 (seen 1 times), 0.03496363026443669 (seen 1 times), 0.14787663729770237 (seen 1 times), 0.11737583070669137 (seen 1 times)
Dictionary with 24 tags: <unk>, 0.0, 0.20091760616484394, 0.01092657937176568, 0.03873252683717413, 0.22925358299492177, 0.01421319865847107, 0.05088806868987229, 0.04255271933823552, 0.15431557436114204, 0.033963367381603445, 0.03386415324057118, 0.1683442401340428, 0.13349866779820807, 0.017162442064271227, 0.003968109286082118, 0.03634830386378241, 0.04689317186751587, 0.03496363026443669, 0.14787663729770237, 0.11737583070669137, 0.020842748416879858, 0.6972504121533641, 0.03163033546382873
2024-04-20 23:31:14,790 ----------------------------------------------------------------------------------------------------
2024-04-20 23:31:14,791 Model: "TextClassifier(
  (embeddings): TransformerDocumentEmbeddings(
    (model): BertModel(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(119548, 768)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoder(
        (layer): ModuleList(
          (0): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
  )
  (decoder): Linear(in_features=768, out_features=24, bias=True)
  (dropout): Dropout(p=0.0, inplace=False)
  (locked_dropout): LockedDropout(p=0.0)
  (word_dropout): WordDropout(p=0.0)
  (loss_function): CrossEntropyLoss()
  (weights): None
  (weight_tensor) None
)"
2024-04-20 23:31:14,791 ----------------------------------------------------------------------------------------------------
2024-04-20 23:31:14,791 Corpus: "Corpus: 6734 train + 748 dev + 15510 test sentences"
2024-04-20 23:31:14,791 ----------------------------------------------------------------------------------------------------
2024-04-20 23:31:14,791 Parameters:
2024-04-20 23:31:14,791  - learning_rate: "0.001000"
2024-04-20 23:31:14,791  - mini_batch_size: "16"
2024-04-20 23:31:14,791  - patience: "2"
2024-04-20 23:31:14,791  - anneal_factor: "0.5"
2024-04-20 23:31:14,791  - max_epochs: "10"
2024-04-20 23:31:14,791  - shuffle: "True"
2024-04-20 23:31:14,791  - train_with_dev: "True"
2024-04-20 23:31:14,791  - batch_growth_annealing: "False"
2024-04-20 23:31:14,791 ----------------------------------------------------------------------------------------------------
2024-04-20 23:31:14,791 Model training base path: "resources/2024-04-20_23-15-33__bert_classifier_on_PAN12_with_seq-len-512/non_quantized"
2024-04-20 23:31:14,792 ----------------------------------------------------------------------------------------------------
2024-04-20 23:31:14,792 Device: cuda:0
2024-04-20 23:31:14,792 ----------------------------------------------------------------------------------------------------
2024-04-20 23:31:14,792 Embeddings storage mode: cpu
2024-04-20 23:31:14,792 ----------------------------------------------------------------------------------------------------
2024-04-20 23:31:19,281 epoch 1 - iter 46/468 - loss 1.83344746 - time (sec): 4.49 - samples/sec: 163.93 - lr: 0.001000
2024-04-20 23:31:23,393 epoch 1 - iter 92/468 - loss 0.93833004 - time (sec): 8.60 - samples/sec: 171.14 - lr: 0.001000
2024-04-20 23:31:27,502 epoch 1 - iter 138/468 - loss 0.63279403 - time (sec): 12.71 - samples/sec: 173.72 - lr: 0.001000
2024-04-20 23:31:31,621 epoch 1 - iter 184/468 - loss 0.49432140 - time (sec): 16.83 - samples/sec: 174.93 - lr: 0.001000
2024-04-20 23:31:35,735 epoch 1 - iter 230/468 - loss 0.40702992 - time (sec): 20.94 - samples/sec: 175.71 - lr: 0.001000
2024-04-20 23:31:39,846 epoch 1 - iter 276/468 - loss 0.34905755 - time (sec): 25.05 - samples/sec: 176.25 - lr: 0.001000
2024-04-20 23:31:43,953 epoch 1 - iter 322/468 - loss 0.30209690 - time (sec): 29.16 - samples/sec: 176.67 - lr: 0.001000
2024-04-20 23:31:48,078 epoch 1 - iter 368/468 - loss 0.26692341 - time (sec): 33.29 - samples/sec: 176.89 - lr: 0.001000
2024-04-20 23:31:52,197 epoch 1 - iter 414/468 - loss 0.24552776 - time (sec): 37.41 - samples/sec: 177.09 - lr: 0.001000
2024-04-20 23:31:56,318 epoch 1 - iter 460/468 - loss 0.22783517 - time (sec): 41.53 - samples/sec: 177.24 - lr: 0.001000
2024-04-20 23:31:57,021 ----------------------------------------------------------------------------------------------------
2024-04-20 23:31:57,021 EPOCH 1 done: loss 0.2253 - lr 0.001000
2024-04-20 23:31:59,846 Evaluating as a multi-label problem: False
2024-04-20 23:31:59,908 TRAIN : loss 0.04273536056280136 - f1-score (micro avg)  0.9967
2024-04-20 23:35:21,690 Evaluating as a multi-label problem: False
2024-04-20 23:35:21,743 TEST : loss 6.834436893463135 - f1-score (micro avg)  0.0
2024-04-20 23:36:20,006 BAD EPOCHS (no improvement): 0
2024-04-20 23:36:51,478 saving best model
2024-04-20 23:37:22,978 ----------------------------------------------------------------------------------------------------
2024-04-20 23:37:27,677 epoch 2 - iter 46/468 - loss 0.05003757 - time (sec): 4.70 - samples/sec: 156.64 - lr: 0.001000
2024-04-20 23:37:31,868 epoch 2 - iter 92/468 - loss 0.05078518 - time (sec): 8.89 - samples/sec: 165.60 - lr: 0.001000
2024-04-20 23:37:36,060 epoch 2 - iter 138/468 - loss 0.04629877 - time (sec): 13.08 - samples/sec: 168.79 - lr: 0.001000
2024-04-20 23:37:40,244 epoch 2 - iter 184/468 - loss 0.03932559 - time (sec): 17.27 - samples/sec: 170.51 - lr: 0.001000
2024-04-20 23:37:44,429 epoch 2 - iter 230/468 - loss 0.04196407 - time (sec): 21.45 - samples/sec: 171.56 - lr: 0.001000
2024-04-20 23:37:48,622 epoch 2 - iter 276/468 - loss 0.03826953 - time (sec): 25.64 - samples/sec: 172.21 - lr: 0.001000
2024-04-20 23:37:52,809 epoch 2 - iter 322/468 - loss 0.03811620 - time (sec): 29.83 - samples/sec: 172.71 - lr: 0.001000
2024-04-20 23:37:57,001 epoch 2 - iter 368/468 - loss 0.03778089 - time (sec): 34.02 - samples/sec: 173.06 - lr: 0.001000
2024-04-20 23:38:01,194 epoch 2 - iter 414/468 - loss 0.04107596 - time (sec): 38.22 - samples/sec: 173.33 - lr: 0.001000
2024-04-20 23:38:05,365 epoch 2 - iter 460/468 - loss 0.04325960 - time (sec): 42.39 - samples/sec: 173.64 - lr: 0.001000
2024-04-20 23:38:06,074 ----------------------------------------------------------------------------------------------------
2024-04-20 23:38:06,074 EPOCH 2 done: loss 0.0439 - lr 0.001000
2024-04-20 23:38:09,009 Evaluating as a multi-label problem: False
2024-04-20 23:38:09,065 TRAIN : loss 0.0420532152056694 - f1-score (micro avg)  0.9967
2024-04-20 23:41:24,507 Evaluating as a multi-label problem: False
2024-04-20 23:41:24,562 TEST : loss 10.231831550598145 - f1-score (micro avg)  0.0
2024-04-20 23:42:22,354 BAD EPOCHS (no improvement): 0
2024-04-20 23:42:51,068 saving best model
2024-04-20 23:43:24,687 ----------------------------------------------------------------------------------------------------
2024-04-20 23:43:29,307 epoch 3 - iter 46/468 - loss 0.07953327 - time (sec): 4.62 - samples/sec: 159.30 - lr: 0.001000
2024-04-20 23:43:33,489 epoch 3 - iter 92/468 - loss 0.06678759 - time (sec): 8.80 - samples/sec: 167.23 - lr: 0.001000
2024-04-20 23:43:37,664 epoch 3 - iter 138/468 - loss 0.04477928 - time (sec): 12.98 - samples/sec: 170.15 - lr: 0.001000
2024-04-20 23:43:41,850 epoch 3 - iter 184/468 - loss 0.05420372 - time (sec): 17.16 - samples/sec: 171.53 - lr: 0.001000
2024-04-20 23:43:46,038 epoch 3 - iter 230/468 - loss 0.05603548 - time (sec): 21.35 - samples/sec: 172.35 - lr: 0.001000
2024-04-20 23:43:50,218 epoch 3 - iter 276/468 - loss 0.04981925 - time (sec): 25.53 - samples/sec: 172.96 - lr: 0.001000
2024-04-20 23:43:54,404 epoch 3 - iter 322/468 - loss 0.04922319 - time (sec): 29.72 - samples/sec: 173.37 - lr: 0.001000
2024-04-20 23:43:58,586 epoch 3 - iter 368/468 - loss 0.04477299 - time (sec): 33.90 - samples/sec: 173.69 - lr: 0.001000
2024-04-20 23:44:02,769 epoch 3 - iter 414/468 - loss 0.04596113 - time (sec): 38.08 - samples/sec: 173.94 - lr: 0.001000
2024-04-20 23:44:06,950 epoch 3 - iter 460/468 - loss 0.04322528 - time (sec): 42.26 - samples/sec: 174.15 - lr: 0.001000
2024-04-20 23:44:07,661 ----------------------------------------------------------------------------------------------------
2024-04-20 23:44:07,662 EPOCH 3 done: loss 0.0425 - lr 0.001000
2024-04-20 23:44:10,505 Evaluating as a multi-label problem: False
2024-04-20 23:44:10,560 TRAIN : loss 0.04223334789276123 - f1-score (micro avg)  0.9967
2024-04-20 23:47:16,722 Evaluating as a multi-label problem: False
2024-04-20 23:47:16,768 TEST : loss 11.417122840881348 - f1-score (micro avg)  0.0
2024-04-20 23:48:09,583 BAD EPOCHS (no improvement): 0
2024-04-20 23:48:38,088 saving best model
2024-04-20 23:49:09,902 ----------------------------------------------------------------------------------------------------
2024-04-20 23:49:14,148 epoch 4 - iter 46/468 - loss 0.02991489 - time (sec): 4.25 - samples/sec: 173.35 - lr: 0.001000
2024-04-20 23:49:18,377 epoch 4 - iter 92/468 - loss 0.03962394 - time (sec): 8.48 - samples/sec: 173.68 - lr: 0.001000
2024-04-20 23:49:22,603 epoch 4 - iter 138/468 - loss 0.03269109 - time (sec): 12.70 - samples/sec: 173.84 - lr: 0.001000
2024-04-20 23:49:26,837 epoch 4 - iter 184/468 - loss 0.04065749 - time (sec): 16.94 - samples/sec: 173.84 - lr: 0.001000
2024-04-20 23:49:31,070 epoch 4 - iter 230/468 - loss 0.04012373 - time (sec): 21.17 - samples/sec: 173.85 - lr: 0.001000
2024-04-20 23:49:35,313 epoch 4 - iter 276/468 - loss 0.05155873 - time (sec): 25.41 - samples/sec: 173.78 - lr: 0.001000
2024-04-20 23:49:39,547 epoch 4 - iter 322/468 - loss 0.04860621 - time (sec): 29.65 - samples/sec: 173.79 - lr: 0.001000
2024-04-20 23:49:43,780 epoch 4 - iter 368/468 - loss 0.04910214 - time (sec): 33.88 - samples/sec: 173.80 - lr: 0.001000
2024-04-20 23:49:48,006 epoch 4 - iter 414/468 - loss 0.04368674 - time (sec): 38.10 - samples/sec: 173.84 - lr: 0.001000
2024-04-20 23:49:52,240 epoch 4 - iter 460/468 - loss 0.04262142 - time (sec): 42.34 - samples/sec: 173.84 - lr: 0.001000
2024-04-20 23:49:52,960 ----------------------------------------------------------------------------------------------------
2024-04-20 23:49:52,960 EPOCH 4 done: loss 0.0419 - lr 0.001000
2024-04-20 23:49:55,863 Evaluating as a multi-label problem: False
2024-04-20 23:49:55,881 TRAIN : loss 0.04061584919691086 - f1-score (micro avg)  0.9967
2024-04-20 23:53:25,770 Evaluating as a multi-label problem: False
2024-04-20 23:53:25,824 TEST : loss 8.669414520263672 - f1-score (micro avg)  0.0
2024-04-20 23:54:18,894 BAD EPOCHS (no improvement): 0
2024-04-20 23:54:48,629 saving best model
2024-04-20 23:55:22,600 ----------------------------------------------------------------------------------------------------
2024-04-20 23:55:26,842 epoch 5 - iter 46/468 - loss 0.04313213 - time (sec): 4.24 - samples/sec: 173.50 - lr: 0.001000
2024-04-20 23:55:31,059 epoch 5 - iter 92/468 - loss 0.02164078 - time (sec): 8.46 - samples/sec: 174.02 - lr: 0.001000
2024-04-20 23:55:35,285 epoch 5 - iter 138/468 - loss 0.04151446 - time (sec): 12.69 - samples/sec: 174.06 - lr: 0.001000
2024-04-20 23:55:39,514 epoch 5 - iter 184/468 - loss 0.04639335 - time (sec): 16.91 - samples/sec: 174.06 - lr: 0.001000
2024-04-20 23:55:43,735 epoch 5 - iter 230/468 - loss 0.04351015 - time (sec): 21.13 - samples/sec: 174.12 - lr: 0.001000
2024-04-20 23:55:47,961 epoch 5 - iter 276/468 - loss 0.04715208 - time (sec): 25.36 - samples/sec: 174.12 - lr: 0.001000
2024-04-20 23:55:52,183 epoch 5 - iter 322/468 - loss 0.04528586 - time (sec): 29.58 - samples/sec: 174.16 - lr: 0.001000
2024-04-20 23:55:56,403 epoch 5 - iter 368/468 - loss 0.04184156 - time (sec): 33.80 - samples/sec: 174.19 - lr: 0.001000
2024-04-20 23:56:00,627 epoch 5 - iter 414/468 - loss 0.04109807 - time (sec): 38.03 - samples/sec: 174.19 - lr: 0.001000
2024-04-20 23:56:04,849 epoch 5 - iter 460/468 - loss 0.04056675 - time (sec): 42.25 - samples/sec: 174.21 - lr: 0.001000
2024-04-20 23:56:05,568 ----------------------------------------------------------------------------------------------------
2024-04-20 23:56:05,568 EPOCH 5 done: loss 0.0414 - lr 0.001000
2024-04-20 23:56:08,445 Evaluating as a multi-label problem: False
2024-04-20 23:56:08,462 TRAIN : loss 0.038309499621391296 - f1-score (micro avg)  0.9967
2024-04-20 23:59:25,154 Evaluating as a multi-label problem: False
2024-04-20 23:59:25,200 TEST : loss 9.422877311706543 - f1-score (micro avg)  0.0
2024-04-21 00:00:18,772 BAD EPOCHS (no improvement): 0
2024-04-21 00:00:47,803 saving best model
2024-04-21 00:01:17,691 ----------------------------------------------------------------------------------------------------
2024-04-21 00:01:21,914 epoch 6 - iter 46/468 - loss 0.01643018 - time (sec): 4.22 - samples/sec: 174.28 - lr: 0.001000
2024-04-21 00:01:26,131 epoch 6 - iter 92/468 - loss 0.05380996 - time (sec): 8.44 - samples/sec: 174.41 - lr: 0.001000
2024-04-21 00:01:30,364 epoch 6 - iter 138/468 - loss 0.05748964 - time (sec): 12.67 - samples/sec: 174.23 - lr: 0.001000
2024-04-21 00:01:34,573 epoch 6 - iter 184/468 - loss 0.04758975 - time (sec): 16.88 - samples/sec: 174.38 - lr: 0.001000
2024-04-21 00:01:38,782 epoch 6 - iter 230/468 - loss 0.03815005 - time (sec): 21.09 - samples/sec: 174.48 - lr: 0.001000
2024-04-21 00:01:42,993 epoch 6 - iter 276/468 - loss 0.03744472 - time (sec): 25.30 - samples/sec: 174.53 - lr: 0.001000
2024-04-21 00:01:47,204 epoch 6 - iter 322/468 - loss 0.03475772 - time (sec): 29.51 - samples/sec: 174.57 - lr: 0.001000
2024-04-21 00:01:51,415 epoch 6 - iter 368/468 - loss 0.03479307 - time (sec): 33.72 - samples/sec: 174.59 - lr: 0.001000
2024-04-21 00:01:55,630 epoch 6 - iter 414/468 - loss 0.03754633 - time (sec): 37.94 - samples/sec: 174.59 - lr: 0.001000
2024-04-21 00:02:02,502 epoch 6 - iter 460/468 - loss 0.04013983 - time (sec): 44.81 - samples/sec: 164.24 - lr: 0.001000
2024-04-21 00:02:03,221 ----------------------------------------------------------------------------------------------------
2024-04-21 00:02:03,221 EPOCH 6 done: loss 0.0410 - lr 0.001000
2024-04-21 00:02:06,093 Evaluating as a multi-label problem: False
2024-04-21 00:02:06,111 TRAIN : loss 0.03474750369787216 - f1-score (micro avg)  0.9967
2024-04-21 00:05:21,915 Evaluating as a multi-label problem: False
2024-04-21 00:05:21,961 TEST : loss 8.945940971374512 - f1-score (micro avg)  0.0
2024-04-21 00:06:15,182 BAD EPOCHS (no improvement): 0
2024-04-21 00:06:49,708 saving best model
2024-04-21 00:07:20,803 ----------------------------------------------------------------------------------------------------
2024-04-21 00:07:24,991 epoch 7 - iter 46/468 - loss 0.03049362 - time (sec): 4.19 - samples/sec: 175.77 - lr: 0.001000
2024-04-21 00:07:29,199 epoch 7 - iter 92/468 - loss 0.03085967 - time (sec): 8.40 - samples/sec: 175.32 - lr: 0.001000
2024-04-21 00:07:33,416 epoch 7 - iter 138/468 - loss 0.04640941 - time (sec): 12.61 - samples/sec: 175.06 - lr: 0.001000
2024-04-21 00:07:37,623 epoch 7 - iter 184/468 - loss 0.03867304 - time (sec): 16.82 - samples/sec: 175.03 - lr: 0.001000
2024-04-21 00:07:41,837 epoch 7 - iter 230/468 - loss 0.04022219 - time (sec): 21.03 - samples/sec: 174.96 - lr: 0.001000
2024-04-21 00:07:46,049 epoch 7 - iter 276/468 - loss 0.04125028 - time (sec): 25.25 - samples/sec: 174.92 - lr: 0.001000
2024-04-21 00:07:50,273 epoch 7 - iter 322/468 - loss 0.04656078 - time (sec): 29.47 - samples/sec: 174.82 - lr: 0.001000
2024-04-21 00:07:54,488 epoch 7 - iter 368/468 - loss 0.04657131 - time (sec): 33.68 - samples/sec: 174.80 - lr: 0.001000
2024-04-21 00:07:58,697 epoch 7 - iter 414/468 - loss 0.04332359 - time (sec): 37.89 - samples/sec: 174.80 - lr: 0.001000
2024-04-21 00:08:02,907 epoch 7 - iter 460/468 - loss 0.04072859 - time (sec): 42.10 - samples/sec: 174.81 - lr: 0.001000
2024-04-21 00:08:03,623 ----------------------------------------------------------------------------------------------------
2024-04-21 00:08:03,623 EPOCH 7 done: loss 0.0401 - lr 0.001000
2024-04-21 00:08:06,485 Evaluating as a multi-label problem: False
2024-04-21 00:08:06,503 TRAIN : loss 0.03950805589556694 - f1-score (micro avg)  0.9967
2024-04-21 00:11:22,210 Evaluating as a multi-label problem: False
2024-04-21 00:11:22,256 TEST : loss 11.014364242553711 - f1-score (micro avg)  0.0
2024-04-21 00:12:15,423 BAD EPOCHS (no improvement): 0
2024-04-21 00:12:50,322 saving best model
2024-04-21 00:13:19,043 ----------------------------------------------------------------------------------------------------
2024-04-21 00:13:23,270 epoch 8 - iter 46/468 - loss 0.05910184 - time (sec): 4.23 - samples/sec: 174.11 - lr: 0.001000
2024-04-21 00:13:27,500 epoch 8 - iter 92/468 - loss 0.04454242 - time (sec): 8.46 - samples/sec: 174.06 - lr: 0.001000
2024-04-21 00:13:31,733 epoch 8 - iter 138/468 - loss 0.04447468 - time (sec): 12.69 - samples/sec: 174.00 - lr: 0.001000
2024-04-21 00:13:35,963 epoch 8 - iter 184/468 - loss 0.04200574 - time (sec): 16.92 - samples/sec: 174.00 - lr: 0.001000
2024-04-21 00:13:40,135 epoch 8 - iter 230/468 - loss 0.03370024 - time (sec): 21.09 - samples/sec: 174.48 - lr: 0.001000
2024-04-21 00:13:44,305 epoch 8 - iter 276/468 - loss 0.03865607 - time (sec): 25.26 - samples/sec: 174.81 - lr: 0.001000
2024-04-21 00:13:48,466 epoch 8 - iter 322/468 - loss 0.03325370 - time (sec): 29.42 - samples/sec: 175.10 - lr: 0.001000
2024-04-21 00:13:52,633 epoch 8 - iter 368/468 - loss 0.03352474 - time (sec): 33.59 - samples/sec: 175.29 - lr: 0.001000
2024-04-21 00:13:56,802 epoch 8 - iter 414/468 - loss 0.03514348 - time (sec): 37.76 - samples/sec: 175.43 - lr: 0.001000
2024-04-21 00:14:00,975 epoch 8 - iter 460/468 - loss 0.04121269 - time (sec): 41.93 - samples/sec: 175.52 - lr: 0.001000
2024-04-21 00:14:01,683 ----------------------------------------------------------------------------------------------------
2024-04-21 00:14:01,683 EPOCH 8 done: loss 0.0406 - lr 0.001000
2024-04-21 00:14:04,514 Evaluating as a multi-label problem: False
2024-04-21 00:14:04,531 TRAIN : loss 0.03479892387986183 - f1-score (micro avg)  0.9967
2024-04-21 00:17:28,731 Evaluating as a multi-label problem: False
2024-04-21 00:17:28,776 TEST : loss 9.896093368530273 - f1-score (micro avg)  0.0
2024-04-21 00:18:21,737 BAD EPOCHS (no improvement): 1
2024-04-21 00:18:56,511 ----------------------------------------------------------------------------------------------------
2024-04-21 00:19:00,716 epoch 9 - iter 46/468 - loss 0.03074499 - time (sec): 4.21 - samples/sec: 175.02 - lr: 0.001000
2024-04-21 00:19:04,922 epoch 9 - iter 92/468 - loss 0.03017430 - time (sec): 8.41 - samples/sec: 175.01 - lr: 0.001000
2024-04-21 00:19:09,125 epoch 9 - iter 138/468 - loss 0.02018982 - time (sec): 12.61 - samples/sec: 175.04 - lr: 0.001000
2024-04-21 00:19:13,334 epoch 9 - iter 184/468 - loss 0.02301160 - time (sec): 16.82 - samples/sec: 175.00 - lr: 0.001000
2024-04-21 00:19:17,548 epoch 9 - iter 230/468 - loss 0.03339945 - time (sec): 21.04 - samples/sec: 174.93 - lr: 0.001000
2024-04-21 00:19:21,762 epoch 9 - iter 276/468 - loss 0.03809463 - time (sec): 25.25 - samples/sec: 174.89 - lr: 0.001000
2024-04-21 00:19:25,972 epoch 9 - iter 322/468 - loss 0.03996879 - time (sec): 29.46 - samples/sec: 174.88 - lr: 0.001000
2024-04-21 00:19:30,187 epoch 9 - iter 368/468 - loss 0.04044622 - time (sec): 33.68 - samples/sec: 174.84 - lr: 0.001000
2024-04-21 00:19:34,410 epoch 9 - iter 414/468 - loss 0.04140004 - time (sec): 37.90 - samples/sec: 174.78 - lr: 0.001000
2024-04-21 00:19:38,616 epoch 9 - iter 460/468 - loss 0.04051435 - time (sec): 42.10 - samples/sec: 174.80 - lr: 0.001000
2024-04-21 00:19:39,333 ----------------------------------------------------------------------------------------------------
2024-04-21 00:19:39,333 EPOCH 9 done: loss 0.0399 - lr 0.001000
2024-04-21 00:19:42,194 Evaluating as a multi-label problem: False
2024-04-21 00:19:42,211 TRAIN : loss 0.03653942048549652 - f1-score (micro avg)  0.9967
2024-04-21 00:22:57,114 Evaluating as a multi-label problem: False
2024-04-21 00:22:57,160 TEST : loss 10.032797813415527 - f1-score (micro avg)  0.0
2024-04-21 00:23:50,123 BAD EPOCHS (no improvement): 0
2024-04-21 00:24:21,520 saving best model
2024-04-21 00:24:51,966 ----------------------------------------------------------------------------------------------------
2024-04-21 00:24:56,181 epoch 10 - iter 46/468 - loss 0.02685157 - time (sec): 4.21 - samples/sec: 174.62 - lr: 0.001000
2024-04-21 00:25:00,400 epoch 10 - iter 92/468 - loss 0.02924166 - time (sec): 8.43 - samples/sec: 174.53 - lr: 0.001000
2024-04-21 00:25:04,623 epoch 10 - iter 138/468 - loss 0.03404686 - time (sec): 12.66 - samples/sec: 174.45 - lr: 0.001000
2024-04-21 00:25:08,834 epoch 10 - iter 184/468 - loss 0.04067679 - time (sec): 16.87 - samples/sec: 174.53 - lr: 0.001000
2024-04-21 00:25:13,043 epoch 10 - iter 230/468 - loss 0.03605765 - time (sec): 21.08 - samples/sec: 174.60 - lr: 0.001000
2024-04-21 00:25:17,259 epoch 10 - iter 276/468 - loss 0.04068159 - time (sec): 25.29 - samples/sec: 174.59 - lr: 0.001000
2024-04-21 00:25:21,474 epoch 10 - iter 322/468 - loss 0.04190027 - time (sec): 29.51 - samples/sec: 174.59 - lr: 0.001000
2024-04-21 00:25:25,689 epoch 10 - iter 368/468 - loss 0.04276646 - time (sec): 33.72 - samples/sec: 174.60 - lr: 0.001000
2024-04-21 00:25:29,904 epoch 10 - iter 414/468 - loss 0.04330895 - time (sec): 37.94 - samples/sec: 174.60 - lr: 0.001000
2024-04-21 00:25:34,114 epoch 10 - iter 460/468 - loss 0.04074841 - time (sec): 42.15 - samples/sec: 174.62 - lr: 0.001000
2024-04-21 00:25:34,831 ----------------------------------------------------------------------------------------------------
2024-04-21 00:25:34,831 EPOCH 10 done: loss 0.0401 - lr 0.001000
2024-04-21 00:25:37,691 Evaluating as a multi-label problem: False
2024-04-21 00:25:37,708 TRAIN : loss 0.03886176273226738 - f1-score (micro avg)  0.9967
2024-04-21 00:28:57,150 Evaluating as a multi-label problem: False
2024-04-21 00:28:57,196 TEST : loss 11.182320594787598 - f1-score (micro avg)  0.0
2024-04-21 00:29:50,359 BAD EPOCHS (no improvement): 1
2024-04-21 00:30:50,868 ----------------------------------------------------------------------------------------------------
2024-04-21 00:34:37,858 Evaluating as a multi-label problem: False
2024-04-21 00:34:37,935 0.0	0.0	0.0	0.0
2024-04-21 00:34:37,935 
Results:
- F-score (micro) 0.0
- F-score (macro) 0.0
- Accuracy 0.0

By class:
               precision    recall  f1-score   support

          0.0     0.0000    0.0000    0.0000       0.0
missing_label     0.0000    0.0000    0.0000    8918.0
     predator     0.0000    0.0000    0.0000    3742.0
 non-predator     0.0000    0.0000    0.0000    2849.0
        label     0.0000    0.0000    0.0000       1.0

     accuracy                         0.0000   15510.0
    macro avg     0.0000    0.0000    0.0000   15510.0
 weighted avg     0.0000    0.0000    0.0000   15510.0

2024-04-21 00:34:37,935 ----------------------------------------------------------------------------------------------------
2024-04-21 00:35:09,454 Loss and F1 plots are saved in resources/2024-04-20_23-15-33__bert_classifier_on_PAN12_with_seq-len-512/non_quantized/training.png
2024-04-21 00:35:09,619 Weights plots are saved in resources/2024-04-20_23-15-33__bert_classifier_on_PAN12_with_seq-len-512/non_quantized/weights.png
