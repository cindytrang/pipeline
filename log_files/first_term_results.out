
---            Run settings            ---
     model_indicator: bert_classifier
             seq_len: 100
   dataset_indicator: Corpus
             project: flair
              run_id: 2024-02-17_00-40-11__bert_classifier_on_PAN12_with_seq-len-100
            data_dir: datasets/Corpus/
             run_dir: resources/2024-02-17_00-40-11__bert_classifier_on_PAN12_with_seq-len-100/
2024-02-17 00:40:11,613 Reading data from datasets/Corpus
2024-02-17 00:40:11,613 Train: datasets/Corpus/PAN12-train.csv
2024-02-17 00:40:11,613 Dev: None
2024-02-17 00:40:11,613 Test: datasets/Corpus/PAN12-test.csv
2024-02-17 00:40:13,805 Filtering empty sentences
2024-02-17 00:40:18,896 Warning: An empty Sentence was created! Are there empty strings in your dataset?
2024-02-17 00:40:27,893 Warning: An empty Sentence was created! Are there empty strings in your dataset?
2024-02-17 00:40:29,148 Corpus: 3012 train + 335 dev + 7754 test sentences
2024-02-17 00:41:13,830 Computing label dictionary. Progress:
2024-02-17 00:41:17,084 Dictionary created for label 'class' with 2 values: non-predator (seen 2911 times), predator (seen 101 times)
Dictionary with 2 tags: non-predator, predator
2024-02-17 00:41:28,258 ----------------------------------------------------------------------------------------------------
2024-02-17 00:41:28,260 Model: "TextClassifier(
  (embeddings): TransformerDocumentEmbeddings(
    (model): BertModel(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(30523, 768)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoder(
        (layer): ModuleList(
          (0): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
  )
  (decoder): Linear(in_features=768, out_features=2, bias=True)
  (dropout): Dropout(p=0.0, inplace=False)
  (locked_dropout): LockedDropout(p=0.0)
  (word_dropout): WordDropout(p=0.0)
  (loss_function): CrossEntropyLoss()
  (weights): None
  (weight_tensor) None
)"
2024-02-17 00:41:28,260 ----------------------------------------------------------------------------------------------------
2024-02-17 00:41:28,260 Corpus: "Corpus: 3012 train + 335 dev + 7754 test sentences"
2024-02-17 00:41:28,260 ----------------------------------------------------------------------------------------------------
2024-02-17 00:41:28,260 Parameters:
2024-02-17 00:41:28,260  - learning_rate: "0.000030"
2024-02-17 00:41:28,260  - mini_batch_size: "16"
2024-02-17 00:41:28,260  - patience: "3"
2024-02-17 00:41:28,260  - anneal_factor: "0.5"
2024-02-17 00:41:28,260  - max_epochs: "3"
2024-02-17 00:41:28,260  - shuffle: "True"
2024-02-17 00:41:28,260  - train_with_dev: "True"
2024-02-17 00:41:28,260  - batch_growth_annealing: "False"
2024-02-17 00:41:28,260 ----------------------------------------------------------------------------------------------------
2024-02-17 00:41:28,260 Model training base path: "resources/2024-02-17_00-40-11__bert_classifier_on_PAN12_with_seq-len-100/non_quantized"
2024-02-17 00:41:28,260 ----------------------------------------------------------------------------------------------------
2024-02-17 00:41:28,260 Device: cuda:0
2024-02-17 00:41:28,260 ----------------------------------------------------------------------------------------------------
2024-02-17 00:41:28,260 Embeddings storage mode: cpu
2024-02-17 00:41:28,260 ----------------------------------------------------------------------------------------------------
2024-02-17 00:41:36,802 epoch 1 - iter 21/210 - loss 0.26820183 - time (sec): 8.54 - samples/sec: 39.34 - lr: 0.000030
2024-02-17 00:41:42,713 epoch 1 - iter 42/210 - loss 0.25964482 - time (sec): 14.45 - samples/sec: 46.50 - lr: 0.000030
2024-02-17 00:41:50,193 epoch 1 - iter 63/210 - loss 0.24981292 - time (sec): 21.93 - samples/sec: 45.96 - lr: 0.000030
2024-02-17 00:41:56,138 epoch 1 - iter 84/210 - loss 0.24339229 - time (sec): 27.88 - samples/sec: 48.21 - lr: 0.000030
2024-02-17 00:42:02,126 epoch 1 - iter 105/210 - loss 0.23714842 - time (sec): 33.87 - samples/sec: 49.61 - lr: 0.000030
2024-02-17 00:42:09,079 epoch 1 - iter 126/210 - loss 0.23269129 - time (sec): 40.82 - samples/sec: 49.39 - lr: 0.000030
2024-02-17 00:42:15,769 epoch 1 - iter 147/210 - loss 0.22252035 - time (sec): 47.51 - samples/sec: 49.51 - lr: 0.000030
2024-02-17 00:42:22,683 epoch 1 - iter 168/210 - loss 0.21881852 - time (sec): 54.42 - samples/sec: 49.39 - lr: 0.000030
2024-02-17 00:42:29,138 epoch 1 - iter 189/210 - loss 0.21242038 - time (sec): 60.88 - samples/sec: 49.67 - lr: 0.000030
2024-02-17 00:42:36,109 epoch 1 - iter 210/210 - loss 0.21013350 - time (sec): 67.85 - samples/sec: 49.33 - lr: 0.000030
2024-02-17 00:42:36,110 ----------------------------------------------------------------------------------------------------
2024-02-17 00:42:36,110 EPOCH 1 done: loss 0.2101 - lr 0.000030
2024-02-17 00:43:11,937 Evaluating as a multi-label problem: False
2024-02-17 00:43:11,959 TRAIN : loss 0.16830801963806152 - f1-score (micro avg)  0.9665
2024-02-17 00:44:56,360 Evaluating as a multi-label problem: False
2024-02-17 00:44:56,388 TEST : loss 0.14662355184555054 - f1-score (micro avg)  0.975
2024-02-17 00:45:09,072 BAD EPOCHS (no improvement): 0
2024-02-17 00:45:09,097 ----------------------------------------------------------------------------------------------------
2024-02-17 00:45:16,602 epoch 2 - iter 21/210 - loss 0.19905478 - time (sec): 7.51 - samples/sec: 44.77 - lr: 0.000030
2024-02-17 00:45:23,231 epoch 2 - iter 42/210 - loss 0.20321415 - time (sec): 14.13 - samples/sec: 47.55 - lr: 0.000030
2024-02-17 00:45:29,003 epoch 2 - iter 63/210 - loss 0.19137167 - time (sec): 19.91 - samples/sec: 50.64 - lr: 0.000030
2024-02-17 00:45:35,954 epoch 2 - iter 84/210 - loss 0.17544191 - time (sec): 26.86 - samples/sec: 50.04 - lr: 0.000030
2024-02-17 00:45:43,133 epoch 2 - iter 105/210 - loss 0.17030805 - time (sec): 34.04 - samples/sec: 49.36 - lr: 0.000030
2024-02-17 00:45:49,926 epoch 2 - iter 126/210 - loss 0.15787006 - time (sec): 40.83 - samples/sec: 49.38 - lr: 0.000030
2024-02-17 00:45:56,139 epoch 2 - iter 147/210 - loss 0.15959669 - time (sec): 47.04 - samples/sec: 50.00 - lr: 0.000030
2024-02-17 00:46:03,166 epoch 2 - iter 168/210 - loss 0.15904630 - time (sec): 54.07 - samples/sec: 49.71 - lr: 0.000030
2024-02-17 00:46:11,453 epoch 2 - iter 189/210 - loss 0.16051499 - time (sec): 62.36 - samples/sec: 48.50 - lr: 0.000030
2024-02-17 00:46:18,133 epoch 2 - iter 210/210 - loss 0.16438731 - time (sec): 69.04 - samples/sec: 48.48 - lr: 0.000030
2024-02-17 00:46:18,134 ----------------------------------------------------------------------------------------------------
2024-02-17 00:46:18,134 EPOCH 2 done: loss 0.1644 - lr 0.000030
2024-02-17 00:46:54,437 Evaluating as a multi-label problem: False
2024-02-17 00:46:54,450 TRAIN : loss 0.14943593740463257 - f1-score (micro avg)  0.9665
2024-02-17 00:48:39,122 Evaluating as a multi-label problem: False
2024-02-17 00:48:39,151 TEST : loss 0.1247824877500534 - f1-score (micro avg)  0.975
2024-02-17 00:48:51,798 BAD EPOCHS (no improvement): 0
2024-02-17 00:48:51,823 ----------------------------------------------------------------------------------------------------
2024-02-17 00:48:58,149 epoch 3 - iter 21/210 - loss 0.17462292 - time (sec): 6.33 - samples/sec: 53.11 - lr: 0.000030
2024-02-17 00:49:05,320 epoch 3 - iter 42/210 - loss 0.18849681 - time (sec): 13.50 - samples/sec: 49.79 - lr: 0.000030
2024-02-17 00:49:11,432 epoch 3 - iter 63/210 - loss 0.19142173 - time (sec): 19.61 - samples/sec: 51.40 - lr: 0.000030
2024-02-17 00:49:17,918 epoch 3 - iter 84/210 - loss 0.18160231 - time (sec): 26.10 - samples/sec: 51.50 - lr: 0.000030
2024-02-17 00:49:26,367 epoch 3 - iter 105/210 - loss 0.17784326 - time (sec): 34.54 - samples/sec: 48.63 - lr: 0.000030
2024-02-17 00:49:32,731 epoch 3 - iter 126/210 - loss 0.17209151 - time (sec): 40.91 - samples/sec: 49.28 - lr: 0.000030
2024-02-17 00:49:39,314 epoch 3 - iter 147/210 - loss 0.16697072 - time (sec): 47.49 - samples/sec: 49.52 - lr: 0.000030
2024-02-17 00:49:46,668 epoch 3 - iter 168/210 - loss 0.15600079 - time (sec): 54.84 - samples/sec: 49.01 - lr: 0.000030
2024-02-17 00:49:53,102 epoch 3 - iter 189/210 - loss 0.15462495 - time (sec): 61.28 - samples/sec: 49.35 - lr: 0.000030
2024-02-17 00:50:01,071 epoch 3 - iter 210/210 - loss 0.15278955 - time (sec): 69.25 - samples/sec: 48.33 - lr: 0.000030
2024-02-17 00:50:01,072 ----------------------------------------------------------------------------------------------------
2024-02-17 00:50:01,072 EPOCH 3 done: loss 0.1528 - lr 0.000030
2024-02-17 00:50:37,477 Evaluating as a multi-label problem: False
2024-02-17 00:50:37,490 TRAIN : loss 0.14524613320827484 - f1-score (micro avg)  0.9665
2024-02-17 00:52:22,041 Evaluating as a multi-label problem: False
2024-02-17 00:52:22,069 TEST : loss 0.11789119243621826 - f1-score (micro avg)  0.975
2024-02-17 00:52:34,536 BAD EPOCHS (no improvement): 0
2024-02-17 00:52:38,216 ----------------------------------------------------------------------------------------------------
2024-02-17 00:52:38,217 Testing using last state of model ...
2024-02-17 00:54:19,361 Evaluating as a multi-label problem: False
2024-02-17 00:54:19,390 0.975	0.975	0.975	0.975
2024-02-17 00:54:19,390 
Results:
- F-score (micro) 0.975
- F-score (macro) 0.4937
- Accuracy 0.975

By class:
              precision    recall  f1-score   support

non-predator     0.9750    1.0000    0.9873      7560
    predator     0.0000    0.0000    0.0000       194

    accuracy                         0.9750      7754
   macro avg     0.4875    0.5000    0.4937      7754
weighted avg     0.9506    0.9750    0.9626      7754

2024-02-17 00:54:19,390 ----------------------------------------------------------------------------------------------------
2024-02-17 00:54:24,648 Loss and F1 plots are saved in resources/2024-02-17_00-40-11__bert_classifier_on_PAN12_with_seq-len-100/non_quantized/training.png
2024-02-17 00:54:24,863 Weights plots are saved in resources/2024-02-17_00-40-11__bert_classifier_on_PAN12_with_seq-len-100/non_quantized/weights.png
